{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0eea92a-2ac8-427e-83d8-61a2ce8c4f41",
   "metadata": {},
   "source": [
    "# cNMF on myeloid cells in atlas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28955de-a531-40b2-ae63-a9b54c24eac7",
   "metadata": {},
   "source": [
    "FROM CNMF PAPER:\n",
    "https://github.com/dylkot/cNMF/blob/main/Tutorials/analyze_pbmc_example_data.ipynb,\n",
    "https://github.com/dylkot/cNMF/blob/main/Tutorials/analyze_batcheffectcorrect_BaronEtAl.ipynb\n",
    "\n",
    "FROM GLIOMA PAPER:\n",
    "https://github.com/BernsteinLab/Myeloid-Glioma/tree/main/Identifying%20recurrent%20programs%20in%20Myeloid%20Cells%20in%20Gliomas%20(Related%20to%20Figure%201)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d332d13d-1327-40ed-b3f1-18e48a67a67a",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b17d6e-c752-4fed-940f-9349bc0f7f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "from cnmf import cNMF, Preprocess\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "np.random.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a90640-9afe-433e-85cf-2a42c769182e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = 'cNMF_w_filtered_genes'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.mkdir(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a938ea-6817-4f01-8fc1-9aed150d475e",
   "metadata": {},
   "source": [
    "## 2. Load filtered counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc83bf4a-433b-437d-9488-32acbd85cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_counts_dir = os.path.join(output_directory, 'filtered_counts.h5ad')\n",
    "adata = sc.read_h5ad(filtered_counts_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b554bb0f-bc83-4354-9ad0-c9757ac33afc",
   "metadata": {},
   "source": [
    "## 3. Correcting counts for batch effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba80a3-24e1-43ce-bd24-a04aba29e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw counts needed\n",
    "adata.layers['norm_counts'] = adata.X.copy()\n",
    "adata.X = adata.layers['counts'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d2944-c901-4eaf-bb5a-638edfe71ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numhvgenes=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd9adeb-3cac-44d2-986c-b09391873cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Preprocess(random_seed=14)\n",
    "(adata_c, adata_tp10k, hvgs) = p.preprocess_for_cnmf(adata, \n",
    "                                                     harmony_vars='dataset', \n",
    "                                                     n_top_rna_genes = numhvgenes,\n",
    "                                                     max_scaled_thresh = None, \n",
    "                                                     quantile_thresh = .9999, \n",
    "                                                     makeplots=False,\n",
    "                                                    save_output_base=os.path.join(output_directory, f\"results\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97270b0f-9218-4c2e-a3ee-438dae2e7396",
   "metadata": {},
   "source": [
    "## 4. Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed79e60d-9604-452c-a557-219c528b7f72",
   "metadata": {},
   "source": [
    "### 4.1 Save counts.h5ad objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64d0a3f-951b-4c5f-9b7e-3bf7f9d06527",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_count_adat_fn = os.path.join(output_directory, 'corrected_counts.h5ad')\n",
    "sc.write(corrected_count_adat_fn, adata_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5669c-eb41-44dc-8a89-cc3a82d8880a",
   "metadata": {},
   "source": [
    "### 4.2 Create a counts.h5ad object for each cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3e9f05-7131-45ea-b130-cd4d72a52461",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = adata_c.obs['dataset'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462a77c-fbb3-4f65-8fc4-a3afc2f63780",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cohort in cohorts:\n",
    "    # Directory for the cohort\n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "    if not os.path.exists(cohort_dir):\n",
    "        os.mkdir(cohort_dir)\n",
    "\n",
    "    adata_cohort = adata_c[adata_c.obs['dataset'] == cohort].copy()\n",
    "\n",
    "    output_h5ad_path = os.path.join(cohort_dir, f\"counts.h5ad\")\n",
    "    adata_cohort.write(output_h5ad_path)\n",
    "\n",
    "    print(f\"✔️ Saved: {output_h5ad_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9125c6a3-7f6a-4466-a55b-fc35009e9f46",
   "metadata": {},
   "source": [
    "## 5. cNMF cohort wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a85b6-0f93-4781-a910-ef643007c15d",
   "metadata": {},
   "source": [
    "### 5.1 Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d5ca9-a167-4ef2-94ca-c008dd442abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of NMF replicates\n",
    "numiter=150 \n",
    "\n",
    "## Number of over-dispersed genes to use for running the actual factorizations\n",
    "numhvgenes=2000 \n",
    "\n",
    "## Specify the Ks to use as a space separated list\n",
    "Ks = np.arange(7,20)\n",
    "\n",
    "## Specify a seed pseudorandom number generation for reproducibility\n",
    "seed = 14 \n",
    "\n",
    "name='cNMF_2000hvg'\n",
    "\n",
    "numworkers = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749ce230-4fba-4d9a-9510-4ee644343486",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_count_adat_fn = os.path.join(output_directory, 'corrected_counts.h5ad')\n",
    "adata = sc.read_h5ad(corrected_count_adat_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800978be-bbac-46fe-a878-8b3b4918707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cohorts = adata.obs['dataset'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a6655-7b0b-4004-b5ab-4d7c59678dd0",
   "metadata": {},
   "source": [
    "### 5.2 Compute cNMF cohort-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211aee6-bbf8-4e4f-9c0a-46aeb8022167",
   "metadata": {},
   "source": [
    "If there are cells with zero over-dispersed genes the function prepare falls into error.\n",
    "Here we implement a new prepare function with filtering of these cells:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f048ea-1669-447e-a95f-96e9e43647da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_cnmf_with_filtering(count_adat_fn, cohort_dir, Ks, numiter, seed, numhvgenes):\n",
    "    cnmf_obj = cNMF(output_dir=cohort_dir, name=name)\n",
    "\n",
    "    try:\n",
    "        cnmf_obj.prepare(\n",
    "            counts_fn=count_adat_fn,\n",
    "            components=Ks,\n",
    "            n_iter=numiter,\n",
    "            seed=seed,\n",
    "            num_highvar_genes=numhvgenes\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"zero counts of overdispersed genes\" in str(e):\n",
    "            print(\"Some cells have zero expression in HVGs. Filtering them using saved HVG list...\")\n",
    "\n",
    "            # Step 1 – load data\n",
    "            adata = sc.read_h5ad(count_adat_fn)\n",
    "            print(f\"Original shape: {adata.shape}\")\n",
    "\n",
    "            # Step 2 – read HVG list from file\n",
    "            hvg_file = os.path.join(cohort_dir, name, f\"{name}.overdispersed_genes.txt\")\n",
    "            if not os.path.isfile(hvg_file):\n",
    "                raise FileNotFoundError(f\"HVG list not found: {hvg_file}\")\n",
    "            with open(hvg_file) as f:\n",
    "                hvg_genes = [line.strip() for line in f if line.strip() in adata.var_names]\n",
    "\n",
    "            print(f\"✔️ Loaded {len(hvg_genes)} HVGs from {hvg_file}\")\n",
    "\n",
    "            # Step 3 – sum expression across HVGs\n",
    "            adata_hvg = adata[:, hvg_genes]\n",
    "            X = adata_hvg.X.toarray() if hasattr(adata_hvg.X, \"toarray\") else adata_hvg.X\n",
    "            cell_sums = np.array(X.sum(axis=1)).flatten()\n",
    "            zero_cells = adata_hvg.obs_names[cell_sums == 0]\n",
    "\n",
    "            print(f\"Found {len(zero_cells)} cells with zero HVG expression.\")\n",
    "\n",
    "            # Step 4 – filter and save\n",
    "            adata_filtered = adata[~adata.obs_names.isin(zero_cells)]\n",
    "            print(f\"Filtered shape: {adata_filtered.shape}\")\n",
    "\n",
    "            filtered_fn = os.path.join(cohort_dir, \"counts_filtered.h5ad\")\n",
    "            adata_filtered.write(filtered_fn)\n",
    "            print(f\"✔️ Saved filtered file to {filtered_fn}\")\n",
    "\n",
    "            # Step 5 – retry prepare\n",
    "            cnmf_obj.prepare(\n",
    "                counts_fn=filtered_fn,\n",
    "                components=Ks,\n",
    "                n_iter=numiter,\n",
    "                seed=seed,\n",
    "                num_highvar_genes=numhvgenes\n",
    "            )\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "    print(\"✔️ Prepare completed.\")\n",
    "    return cnmf_obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb0ffd-e519-4f81-9666-409ae9e2900d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looping on cohorts\n",
    "for cohort in cohorts:\n",
    "    print(f\"\\n--- Computing cNMF for cohort: {cohort} ---\")\n",
    "\n",
    "    # Directory for the cohort\n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "    count_adat_fn = os.path.join(cohort_dir, f\"counts.h5ad\")\n",
    "    \n",
    "    # Initialize the cnmf object that will be used to run analyses\n",
    "    cnmf_obj = cNMF(output_dir=cohort_dir, name=name)\n",
    "\n",
    "    # Prepare the data, I.e. subset to 2000 high-variance genes, and variance normalize\n",
    "    cnmf_obj = prepare_cnmf_with_filtering(\n",
    "        count_adat_fn=count_adat_fn,\n",
    "        cohort_dir=cohort_dir,\n",
    "        Ks=Ks,\n",
    "        numiter=numiter,\n",
    "        seed=seed,\n",
    "        numhvgenes=numhvgenes\n",
    "    )\n",
    "\n",
    "    log_dir = os.path.join(cohort_dir, f\"factorize.log\")\n",
    "    with open(log_dir, \"w\") as log:\n",
    "        with contextlib.redirect_stdout(log), contextlib.redirect_stderr(log):\n",
    "            cnmf_obj.factorize_multi_process(total_workers=numworkers)\n",
    "    print(f\"✔️ Factorize-module completed\")\n",
    "\n",
    "    # combines the individual factorization replicates so that the consensus estimate can be taken\n",
    "    cnmf_obj.combine()\n",
    "    print(f\"✔️ Combine-module completed\")\n",
    "\n",
    "    # Compute the stability and error at each choice of K to see if a clear choice jumps out\n",
    "    cnmf_obj.k_selection_plot(close_fig=False)\n",
    "\n",
    "    print(f\"✔️ cNMF completed for cohort {cohort}. Results in: {cohort_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2838f18-8935-4ab6-a18b-1672056ffb7e",
   "metadata": {},
   "source": [
    "#### 5.2.1 Optimal K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351da5af-dbb8-4f66-b9a0-e48fd8a35b6e",
   "metadata": {},
   "source": [
    "The optimal K parameter is chosen looking at a good compromise between the minimum reconstruction error and maximum stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8956368-69a5-4cba-b483-bad5b3628536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_k_selection_df(npz_path):\n",
    "    npz = np.load(npz_path, allow_pickle=True)\n",
    "    data = npz['data']\n",
    "    columns = npz['columns']\n",
    "    index = npz['index']\n",
    "    \n",
    "    # Ricostruisce il DataFrame\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.index = index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f23c906-be1f-484d-89dc-86443a18a5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_k_combined(df, cohort, weight_silhouette=0.5, plot=True):\n",
    "    \n",
    "    # Normalize silhouette \n",
    "    scaler = MinMaxScaler()\n",
    "    df[\"silhouette_norm\"] = scaler.fit_transform(df[[\"silhouette\"]])\n",
    "    \n",
    "    # Re-sort and normalize prediction_error \n",
    "    df[\"error_inv\"] = -df[\"prediction_error\"]\n",
    "    df[\"error_norm\"] = scaler.fit_transform(df[[\"error_inv\"]])\n",
    "    \n",
    "    # Combined score\n",
    "    df[\"score\"] = weight_silhouette * df[\"silhouette_norm\"] + (1 - weight_silhouette) * df[\"error_norm\"]\n",
    "\n",
    "    # Select best k\n",
    "    best_row = df.loc[df[\"score\"].idxmax()]\n",
    "    best_k = int(best_row[\"k\"])\n",
    "\n",
    "    if plot:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(df[\"k\"], df[\"score\"], label=\"Combined Score\", marker=\"*\")\n",
    "        plt.axvline(best_k, color=\"red\", linestyle=\"--\", label=f\"Best k = {best_k}\")\n",
    "        plt.xlabel(\"k\")\n",
    "        plt.title(f\"Automatic K selection - cohort {cohort}\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"✔️ Best k for cohort {cohort}: {best_k}\")\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c6274-b608-483f-920e-871d15b172eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for cohort in cohorts:\n",
    "    print(f\"\\n--- Finding best K for cohort: {cohort} ---\")\n",
    "\n",
    "    # Directory \n",
    "    npz_path = os.path.join(output_directory, cohort, name, f\"{name}.k_selection_stats.df.npz\")\n",
    "    df = load_k_selection_df(npz_path)\n",
    "\n",
    "    selected_K = select_best_k_combined(df, cohort, weight_silhouette=0.5, plot=True)\n",
    "    \n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "    with open(os.path.join(cohort_dir, name, \"selected_K.txt\"), \"w\") as f:\n",
    "        f.write(str(selected_K) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3784383e-6634-4f16-98f8-2d0fa3b8d21d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "density_threshold = 0.1\n",
    "\n",
    "for cohort in cohorts:\n",
    "    print(f\"\\n--- Running consensus for cohort {cohort} ---\")\n",
    "\n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "\n",
    "    with open(os.path.join(cohort_dir, name, \"selected_K.txt\")) as f:\n",
    "        selected_K = int(f.read().strip())\n",
    "\n",
    "    cnmf_obj = cNMF(output_dir=cohort_dir, name=name)\n",
    "    \n",
    "    cnmf_obj.consensus(k=selected_K, density_threshold=density_threshold,\n",
    "                       show_clustering=True, close_clustergram_fig=False)\n",
    "\n",
    "    print(f\"✔️ Consensus done for {cohort}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ca53ba-44ba-4294-b0ef-4212b66150a5",
   "metadata": {},
   "source": [
    "## 6. Compute Consensus Programs with cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7050e4c7-eab5-4862-a406-83e5fc3f7e32",
   "metadata": {},
   "source": [
    "### 6.1 Cosine similarity matrix (GEPs x GEPs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d24c8f-0ff8-4204-8927-b7fca72d8f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_spectra = []  # DataFrame list\n",
    "program_labels = []  \n",
    "density_threshold = 0.1\n",
    "\n",
    "for cohort in cohorts:\n",
    "    \n",
    "    # load\n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "    with open(os.path.join(cohort_dir, name, \"selected_K.txt\")) as f:\n",
    "        selected_K = int(f.read().strip())\n",
    "    spectra_path = os.path.join(cohort_dir, name,\n",
    "                                f\"{name}.gene_spectra_score.k_{selected_K}.dt_{str(f'{density_threshold}').replace('.', '_')}.txt\")\n",
    "\n",
    "    # create dataframe\n",
    "    df = pd.read_csv(spectra_path, sep=\"\\t\", index_col=0).T # genes x geps\n",
    "    for i in range(df.shape[1]):\n",
    "        all_spectra.append(df.iloc[:, i])\n",
    "        program_labels.append(f\"{cohort}_k{i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f83e7-4a6c-4659-a540-47e3ac9e0bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert geps x genes\n",
    "spectra_matrix = pd.DataFrame(all_spectra).T\n",
    "spectra_matrix.columns = program_labels\n",
    "\n",
    "# cosine similarity (gep x gep)\n",
    "cos_sim_matrix = cosine_similarity(spectra_matrix.T)\n",
    "cos_sim_df = pd.DataFrame(cos_sim_matrix, index=program_labels, columns=program_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac8ab62-54b5-4b19-a127-31e550740329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "directory = os.path.join(output_directory, \"2000hvg\")\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "np.save(f\"{directory}/cosine_similarity_matrix.npy\", cos_sim_matrix)\n",
    "with open(f\"{directory}/program_labels.txt\", \"w\") as f:\n",
    "    for label in program_labels:\n",
    "        f.write(f\"{label}\\n\")\n",
    "spectra_matrix.to_csv(f\"{directory}/spectra_matrix.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de918ef6-dd29-47b3-9139-ea023377a0b7",
   "metadata": {},
   "source": [
    "### 6.2 Hierarchical clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12549d69-4ada-47b5-93f6-610aa81e735b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cos_sim_matrix[cos_sim_matrix < 0] = 0 # negative values to 0\n",
    "\n",
    "linkage_matrix = linkage(1 - cos_sim_matrix, method='ward')\n",
    "\n",
    "# Heatmap \n",
    "sns.clustermap(cos_sim_df, row_linkage=linkage_matrix, col_linkage=linkage_matrix,\n",
    "               cmap=\"Reds\", figsize=(22, 22))\n",
    "\n",
    "plt.suptitle(\"Cosine similarity between GEPs across cohorts (negatives zeroed)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040397dd-1937-44a0-8de1-0b621360d522",
   "metadata": {},
   "source": [
    "### 6.3 Cophenetic distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbab0ea-ba7d-481d-ba30-af450e6de2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 3\n",
    "clusters = fcluster(linkage_matrix, t=threshold, criterion='distance')\n",
    "\n",
    "cluster_dict = defaultdict(list)\n",
    "for label, clust_id in zip(program_labels, clusters):\n",
    "    cluster_dict[clust_id].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1181c7-7b19-43cc-863d-3d3b9a058ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract cohort name using '_k' as stop string\n",
    "cohort_names = [col.split(\"_k\")[0] for col in cos_sim_df.columns]\n",
    "\n",
    "# dictionary: unique color for each cohort\n",
    "unique_cohorts = sorted(set(cohort_names))\n",
    "palette = sns.color_palette(\"tab20\", len(unique_cohorts))  \n",
    "cohort_colors_dict = {cohort: palette[i] for i, cohort in enumerate(unique_cohorts)}\n",
    "\n",
    "# apply colors\n",
    "cohort_colors = [cohort_colors_dict[cohort] for cohort in cohort_names]\n",
    "\n",
    "g = sns.clustermap(cos_sim_df,\n",
    "                   row_linkage=linkage_matrix,\n",
    "                   col_linkage=linkage_matrix,\n",
    "                   row_colors=cohort_colors,\n",
    "                   col_colors=cohort_colors,\n",
    "                   cmap=\"Reds\",\n",
    "                   figsize=(13, 13),\n",
    "                   dendrogram_ratio=(0.0001, 0.0001),  # remove dendrograms\n",
    "                   cbar_pos=None  # disabilita la colorbar automatica\n",
    "                  )\n",
    "\n",
    "g.ax_heatmap.set_xticks([])\n",
    "g.ax_heatmap.set_yticks([])\n",
    "\n",
    "# legend\n",
    "legend_handles = [Patch(facecolor=cohort_colors_dict[c], label=c) for c in unique_cohorts]\n",
    "g.ax_heatmap.legend(handles=legend_handles, title=\"Cohort\", loc='lower right', bbox_to_anchor=(-0.05, 0.0), fontsize=12)\n",
    "\n",
    "# horizontal lines for consensus programs\n",
    "ordered_row_indices = g.dendrogram_row.reordered_ind\n",
    "ordered_clusters = np.array(clusters)[ordered_row_indices]\n",
    "boundaries = np.where(np.diff(ordered_clusters) != 0)[0]\n",
    "for b in boundaries:\n",
    "    g.ax_heatmap.hlines(b + 1, *g.ax_heatmap.get_xlim(), colors='black', linewidth=1.5)\n",
    "\n",
    "# Consensus programs labels\n",
    "boundaries_ext = np.concatenate([[-1], boundaries, [len(ordered_clusters) - 1]])\n",
    "centers = [(boundaries_ext[i] + boundaries_ext[i + 1]) / 2 +1 for i in range(len(boundaries_ext) - 1)]\n",
    "consensus_labels = ['- Cell cycle (activity)', #1\n",
    "                   '- Immunosuppressive response - TAMs/LAMs (activity)', #2\n",
    "                   '- Immunosuppressive response - M2-like Macrophages (identity)', #3\n",
    "                   '- Immunosuppressive Macrophage Metabolism (activity)', #4\n",
    "                   '- Mast cell (identity)', #5\n",
    "                   '- Plasma / B cell activation (activity)', #6\n",
    "                   '- Inflammatory - Neutrophils & Monocytes (mixed)', #7\n",
    "                   '- Epithelial interaction (activity)', #8\n",
    "                   '- cDC (identity)', #9\n",
    "                   '- T cells activation (activity)', #10\n",
    "                   '- Metabolism (activity)', #11\n",
    "                   '- HS-UPR-stress response (activity)', #12\n",
    "                   '- IFN response (activity)', #13\n",
    "                   '- Hypoxia - metallic stress (activity)', #14\n",
    "                   '- EMT (activity)', #15\n",
    "                   '- DC mature (identity)', #16\n",
    "                   '- pDC (identity)'] #17\n",
    "\n",
    "for y, label in zip(centers, consensus_labels):\n",
    "    g.ax_heatmap.text(x=g.ax_heatmap.get_xlim()[1] + 2,\n",
    "                      y=y,\n",
    "                      s=label,\n",
    "                      va='center',\n",
    "                      ha='left',\n",
    "                      fontsize=12)\n",
    "\n",
    "\n",
    "# colorbar\n",
    "norm = plt.Normalize(vmin=cos_sim_df.values.min(), vmax=cos_sim_df.values.max())\n",
    "sm = plt.cm.ScalarMappable(cmap=\"Reds\", norm=norm)\n",
    "sm.set_array([])\n",
    "cbar_ax = g.fig.add_axes([-0.09, 0.55, 0.02, 0.25]) \n",
    "cb = g.fig.colorbar(sm, cax=cbar_ax, orientation='vertical')\n",
    "cbar_ax.tick_params(labelsize=10)\n",
    "cbar_ax.set_title(\"Cosine similarity\", fontsize=12, pad=15)\n",
    "g.fig.colorbar(sm, cax=cbar_ax, orientation='vertical')\n",
    "\n",
    "# layout \n",
    "g.fig.subplots_adjust(left=0.05, right=0.95, top=0.93, bottom=0.1)\n",
    "plt.show()\n",
    "\n",
    "g.fig.savefig(f\"{directory}/clustermap.pdf\", format=\"pdf\", bbox_inches='tight')\n",
    "g.fig.savefig(f\"{directory}/clustermap.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab8053-b0a5-40c6-8f4c-0c103d83753e",
   "metadata": {},
   "source": [
    "### 6.4 Top genes in GEPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd328a5-d340-49ff-b1b0-bf2acdcd3122",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_programs = {}\n",
    "for cluster_id, programs in cluster_dict.items():\n",
    "    if len(programs) < 2:\n",
    "        continue  \n",
    "\n",
    "    # Mean\n",
    "    subset = spectra_matrix[programs]\n",
    "    consensus = subset.mean(axis=1)\n",
    "    consensus_programs[f\"consensus_{cluster_id}\"] = consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0765e5e5-82ea-4945-95c2-cf849f22ede0",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_genes_dict = {}\n",
    "for name, consensus in consensus_programs.items():\n",
    "    # Order genes\n",
    "    top_genes = consensus.sort_values(ascending=False).head(100).index.tolist()\n",
    "    top_genes_dict[name] = top_genes\n",
    "\n",
    "# Trasform in DataFrame \n",
    "consensus_top_genes_df = pd.DataFrame.from_dict(\n",
    "    top_genes_dict, orient=\"index\", columns=[f\"gene_{i+1}\" for i in range(100)]\n",
    ")\n",
    "consensus_top_genes_df = consensus_top_genes_df.T.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd2bf0f-f355-4523-b0de-a5b1e8932098",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# map ENSG -> GeneSymbol\n",
    "gene_map = adata.var[\"GeneSymbol\"].to_dict()\n",
    "\n",
    "top_genes_dict = {}\n",
    "for name, consensus in consensus_programs.items():\n",
    "    # top 100\n",
    "    top_ensg = consensus.sort_values(ascending=False).head(100).index.tolist()\n",
    "\n",
    "    # if symbol not found, use ENSG\n",
    "    top_symbols = [gene_map.get(ensg, ensg) for ensg in top_ensg]\n",
    "    top_genes_dict[name] = top_symbols\n",
    "\n",
    "consensus_top_genes_df = pd.DataFrame.from_dict(\n",
    "    top_genes_dict, orient=\"index\", columns=[f\"gene_{i+1}\" for i in range(100)]\n",
    ")\n",
    "\n",
    "consensus_top_genes_df.T.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca311b-439e-4178-bb6c-2a43eeedc924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "consensus_top_genes_df.T.to_csv(f\"{directory}/cos_similarity_consensus_top_genes_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d5cf0b-1bc0-4434-a9b8-b7bb11682b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map ENSG in symbols\n",
    "consensus_programs_symbols = {} # for gsea\n",
    "\n",
    "for consensus, series in consensus_programs.items():\n",
    "    renamed_series = series.rename(index=lambda ensg: gene_map.get(ensg, ensg))\n",
    "    consensus_programs_symbols[consensus] = renamed_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb45c77-a12e-479c-9d03-bbb3ae4efe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(f\"{directory}/consensus_programs_symbols.pkl\", \"wb\") as f:\n",
    "    pickle.dump(consensus_programs_symbols, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa5a5e2-0a27-49ce-983d-8a791fb2ab2a",
   "metadata": {},
   "source": [
    "## 7. Show programs usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23aae2e-7072-495f-9a22-0bc552ec300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usage = []  # DataFrame list\n",
    "density_threshold = 0.1\n",
    "name = 'cNMF_2000hvg'\n",
    "directory = os.path.join(output_directory, \"2000hvg\")\n",
    "\n",
    "for cohort in cohorts:\n",
    "    labels = []\n",
    "    cohort_dir = os.path.join(output_directory, cohort)\n",
    "    with open(os.path.join(cohort_dir, name, \"selected_K.txt\")) as f:\n",
    "        selected_K = int(f.read().strip())\n",
    "        \n",
    "    usage_path = os.path.join(cohort_dir, name,\n",
    "                                f\"{name}.usages.k_{selected_K}.dt_{str(f'{density_threshold}').replace('.', '_')}.consensus.txt\")\n",
    "\n",
    "    df = pd.read_csv(usage_path, sep=\"\\t\", index_col=0) # cells x geps\n",
    "    for i in range(df.shape[1]):\n",
    "        labels.append(f\"{cohort}_k{i}\")\n",
    "    df.columns = labels\n",
    "    df_normalized = df.div(df.sum(axis=1), axis=0)\n",
    "    all_usage.append(df_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e4d2e9-fdbe-4090-9d31-79311de1e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of DataFrames: {len(all_usage)}\")\n",
    "\n",
    "for i, df in enumerate(all_usage):\n",
    "    print(f\"DataFrame {i}: shape = {df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5b898f-a064-4adc-8a35-f1e522825c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "consensus_to_gep = defaultdict(list)\n",
    "for k, v in cluster_dict.items():\n",
    "    new_key = f\"consensus_{int(k)}\"\n",
    "    consensus_to_gep[new_key] = v\n",
    "\n",
    "# Invert dict: GEP -> consensus\n",
    "gep_to_consensus = {}\n",
    "for consensus, geps in consensus_to_gep.items():\n",
    "    for gep in geps:\n",
    "        gep_to_consensus[gep] = consensus\n",
    "\n",
    "# List of the dataframes cells x consensus\n",
    "aggregated_dfs = []\n",
    "\n",
    "for df in all_usage:\n",
    "    valid_cols = df.columns\n",
    "    df_filtered = df[valid_cols].copy()\n",
    "    # Rename columns with consensus ID\n",
    "    df_filtered.columns = [gep_to_consensus[col] for col in df_filtered.columns]\n",
    "    # Sum columns with same consensus\n",
    "    df_grouped = df_filtered.groupby(df_filtered.columns, axis=1).sum()\n",
    "    aggregated_dfs.append(df_grouped)\n",
    "\n",
    "# Merge in a dataframe\n",
    "usage_consensus = pd.concat(aggregated_dfs, axis=0)\n",
    "# Sum columns with same consensus\n",
    "usage_consensus_grouped = usage_consensus.groupby(usage_consensus.columns, axis=1).sum()\n",
    "\n",
    "# Order according consensus ID\n",
    "usage_consensus_grouped = usage_consensus_grouped.reindex(\n",
    "    sorted(usage_consensus_grouped.columns, key=lambda x: int(x.split('_')[1])),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Row sum to 1\n",
    "usage_consensus_grouped_normalized = usage_consensus_grouped.div(usage_consensus_grouped.sum(axis=1), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a8c4d7-06b8-45b3-9108-3c3f8d4674af",
   "metadata": {},
   "source": [
    "### 7.1 Cell type usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3438e5df-e1fa-477e-b7d3-e0fc0c2eab25",
   "metadata": {},
   "source": [
    "#### 7.1.1 Fine labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6462a7-228d-4100-91e0-834c366ebe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_norm_forplot = usage_consensus_grouped_normalized.unstack().reset_index()\n",
    "usage_norm_forplot = pd.merge(left=usage_norm_forplot, right=adata.obs[['cell_type_fine', 'dataset']], left_on='level_1', right_index=True)\n",
    "usage_norm_forplot.columns = ['GEP', 'cell', 'Usage', 'Cell Type', 'Donor']\n",
    "\n",
    "cluster_order = adata.obs['cell_type_fine'].unique().tolist()\n",
    "\n",
    "(fig,axes) = plt.subplots(len(cluster_order),1, figsize=(20,20), dpi=200, gridspec_kw={'hspace':0.8})\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "\n",
    "for i,k in enumerate(cluster_order):\n",
    "    g = usage_norm_forplot.loc[usage_norm_forplot['Cell Type'] == k, :]\n",
    "    sns.boxplot(x='GEP', y='Usage', hue='Donor', ax=axes[i], data=g, fliersize=1)\n",
    "    axes[i].set_title(k, fontsize=16)\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    axes[i].set_ylim(0, 1)\n",
    "    axes[i].tick_params(axis='y', labelsize=10)\n",
    "    axes[i].legend(bbox_to_anchor=(1,1))\n",
    "    if i != (len(cluster_order)-1):\n",
    "        axes[i].set_xticklabels([])\n",
    "    else:\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right', fontsize=14)\n",
    "        \n",
    "    if i != 0:\n",
    "        axes[i].legend().remove()\n",
    "    else:\n",
    "        axes[i].legend(bbox_to_anchor=(1,1))\n",
    "\n",
    "fig.text(0.5, 0.02, 'Consensus Program', ha='center', fontsize=16)\n",
    "fig.text(0.06, 0.5, 'Normalized Usage', va='center', rotation='vertical', fontsize=16)\n",
    "\n",
    "# Save\n",
    "fig.savefig(f\"{directory}/usage_by_celltype_fine.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "fig.savefig(f\"{directory}/usage_by_celltype_fine.svg\", format=\"svg\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51366be8-589a-46a0-b577-e1925b422e65",
   "metadata": {},
   "source": [
    "#### 7.1.2 Middle labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaffa6d-79c5-429a-a74b-459be1398711",
   "metadata": {},
   "outputs": [],
   "source": [
    "usage_norm_forplot_middle = usage_consensus_grouped_normalized.unstack().reset_index()\n",
    "usage_norm_forplot_middle = pd.merge(left=usage_norm_forplot_middle, right=adata.obs[['cell_type_middle', 'dataset']], left_on='level_1', right_index=True)\n",
    "usage_norm_forplot_middle.columns = ['GEP', 'cell', 'Usage', 'Cell Type', 'Donor']\n",
    "\n",
    "cluster_order = adata.obs['cell_type_middle'].unique().tolist()\n",
    "\n",
    "(fig,axes) = plt.subplots(len(cluster_order),1, figsize=(20,10), dpi=200, gridspec_kw={'hspace':.8})\n",
    "for i,k in enumerate(cluster_order):\n",
    "    g = usage_norm_forplot_middle.loc[usage_norm_forplot_middle['Cell Type'] == k, :]\n",
    "    sns.boxplot(x='GEP', y='Usage', hue='Donor', ax=axes[i], data=g, fliersize=1)\n",
    "    axes[i].set_title(k)\n",
    "    axes[i].legend(bbox_to_anchor=(1,1))\n",
    "    axes[i].set_xlabel('')\n",
    "    axes[i].set_ylabel('')\n",
    "    if i != (len(cluster_order)-1):\n",
    "        axes[i].set_xticklabels([])\n",
    "    else:\n",
    "        axes[i].set_xticklabels(axes[i].get_xticklabels(), rotation=45, ha='right')\n",
    "        \n",
    "    if i != 0:\n",
    "        axes[i].legend().remove()\n",
    "    else:\n",
    "        axes[i].legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2faeae-bc06-44cf-a740-e4bd33caae1e",
   "metadata": {},
   "source": [
    "### 7.2 Cumulative usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbe023-ea40-441b-a404-d109ff55f614",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_usage = usage_consensus_grouped_normalized.sum(axis=0)\n",
    "\n",
    "# order according descending usage\n",
    "total_usage_sorted = total_usage.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=total_usage_sorted.index, y=total_usage_sorted.values, palette='viridis')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Total normalized usage across all cells')\n",
    "plt.xlabel('Consensus programs')\n",
    "plt.title('Cumulative usage of consensus programs')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbea33-6c65-4dac-89fd-9b4032b5c0a6",
   "metadata": {},
   "source": [
    "### 7.3 UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d19ac-edd5-4d23-a41b-9e84731f2db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# order cells as in adata\n",
    "consensus_df_aligned = usage_consensus_grouped_normalized.loc[adata.obs_names]\n",
    "\n",
    "# add consensus in .obs\n",
    "for col in consensus_df_aligned.columns:\n",
    "    adata.obs[col] = consensus_df_aligned[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8634803-b9b9-425e-9241-de07e67cb482",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.normalize_total(adata)\n",
    "sc.pp.log1p(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095f226-7fb3-4cfc-8472-1284efc681a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.layers[\"logcounts\"] = adata.X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec363ea6-50b4-4d9c-a8ce-f67dd1ce174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.pca(adata, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac61d1-c20f-44b7-9d9b-8ef473ee9e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.neighbors(adata, n_neighbors=30, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c54f1e-58d4-461a-8b01-e36fe13a0da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.tl.umap(adata, min_dist=0.1, spread=1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e1230-e74d-41b9-85d7-095763a5c4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color=['dataset','cell_type_fine'], wspace=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f05b2c-0225-45b8-887f-075f0ed32786",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.umap(adata, color=consensus_df_aligned.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (glioma_plot_clone)",
   "language": "python",
   "name": "glioma_plot_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
